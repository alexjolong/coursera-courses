{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 - Anomaly Detection\n",
    "## Density Estimation\n",
    "### Problem Motivation\n",
    "- Anomaly detection is a hybrid between unsupervised and supervised learning.\n",
    "- Application: features of aircraft engines such as heat generated, vibration intensity, etc.\n",
    "- For any new (test) engine, we want to know if it is anomalous in any way.\n",
    "- Need to define some epsilon which is a value, above which a classification will be marked as anomolous.\n",
    "\n",
    "### Gaussian Distribution\n",
    "- AKA normal distribution\n",
    "- I'm pretty sure i know this... but I'll watch anyway (at 1.5x)\n",
    "x ~ N(mu,sigma^2)\n",
    "- This means that x is a distributed Gaussian with mean mu and variance sigma^2\n",
    "p(x; mu, sigma^2) = 1/(sqrt(2* pi) * sigma) * exp(-(x-mu)^2 / 2(sigma^2))\n",
    "- As sigma increases, the distribution becomes \"fatter\"\n",
    "- As mu increases, the x offset of the distribution increases\n",
    "\n",
    "### Algorithm\n",
    "- For a training set {x1, x2, ..., xm}, p(x) = the product of p(xi;mui,sigmai^2) for all i, where i is a feature across the entire set\n",
    "\n",
    "steps:\n",
    "1. Choose features xi that might be indicative of anomalous examples\n",
    "2. Fit parameters mu1, ..., mun, sigma1^2, ..., sigman^2\n",
    "muj = 1/m * SUM(i=1:m)(xj^i)\n",
    "sigmaj^2 = 1/m * SUM(i=1:m)(xj^i - muj)^2\n",
    "3. Given new example x, compute p(x)\n",
    "p(x) = PRODUCT(p(xj; muj, sigmaj^2)\n",
    "4. x is anomalous is p(x) < epsilon\n",
    "\n",
    "\n",
    "## Building an Anomaly Detection System\n",
    "### Developing and Evaluating an Anomaly Detection System\n",
    "- How do we evaluate our algorithm (how well it performs at detecting anomalies)\n",
    "- Assume we have labeled data, y=0 if normal, y=1 if anomalous\n",
    "- We will likely have very sparse anomalous examples\n",
    "- Only put good examples in the training set. Put half of your anomalous examples in CV, half in test.\n",
    "- On CV and test, y=1 if p(x)< epsilon (anomaly), y=0 if p(x) >= epsilon (normal)\n",
    "\n",
    "Possible evaluation metrics: \n",
    "- true positive, false positive, false neg, true neg\n",
    "- Precision/Recall\n",
    "- F1-score\n",
    "\n",
    "Since data is very skewed, classification accuracy is not a good metric. An algorithm that always predicts normal might perform very well, but not when needed.\n",
    "\n",
    "- Note that we can also use the cross validation to test and choose the parameter epsilon.\n",
    "\n",
    "### Anomaly Detection vs Supervised Learning\n",
    "- When to use which?\n",
    "- Anomaly:\n",
    "- - When there is avery small number of positive examples (0-20 is common)\n",
    "- - If there are many different \"types\" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like.\n",
    "- - Common cases: fraud detection, manufacturing, machine monitoring\n",
    "- Supervised learninig:\n",
    "- - When there are a large number of both positive and negative examples\n",
    "- - When there are enough positive examples (anomalies) to learn what they might look like.\n",
    "- - spam classification, weather prediction\n",
    "\n",
    "### Choosing what features to use\n",
    "- If features are non-gaussian, the algorithm will often work just fine, but we would prefer to transform the data.\n",
    "- - for instance, we can take a log(x) transformation.\n",
    "- - you can also take x^a where a is some fraction.\n",
    "\n",
    "- Look at the data and see if it inspires you to create new features.\n",
    "- Features should either take on very large or very small values in the event of an anomaly\n",
    "\n",
    "## Quiz\n",
    "4/5 (80%)\n",
    "\n",
    "\n",
    "# Week 9 - Recommender Systems\n",
    "## Predicting Movie Ratings\n",
    "### Problem Formulation\n",
    "- Recommender systems are some of the most important (i.e. commonly used) applications of machine learning\n",
    "- Responsible for substantial fractions of amazon, netflix, and other companies revenues.\n",
    "- Interestingly, doesn't receive a lot of attention in academic literature.\n",
    "\n",
    "Values in a recommender systems problem:\n",
    "nu = # of users\n",
    "nm = # of movies\n",
    "r(i,j) = 1 if user j has rated movie i\n",
    "y(i,j) = rating given y user j to movie i. Range between 0:5\n",
    "\n",
    "### Content Based Recommendations\n",
    "- Define a set of features which represent movie \"content\". Individual movies might have unique weights for each feature.\n",
    "- n = # of features\n",
    "\n",
    "To make predictions, we could do the following:\n",
    "1. for each user j, learn parameter vector theta(j). (we'll get to how this can be done)\n",
    "2. Predict user j as rating movie i with theta(j)' * x(i) stars\n",
    "\n",
    "Then we just do usual linear regression (using least squares). Except we sum the objective over all users. I.e. two sums - one across all features, one across all users.\n",
    " - Add in regularization, just 4 fun.\n",
    " \n",
    "Then do gradient descent on the derivative of our objective * a learning rate.\n",
    "\n",
    "## Collaborative Filtering\n",
    "### Collaborative Filtering\n",
    "- Algorithms can learn for themselves what features to use.\n",
    "- This requires us to build the theta matrix beforehand - possibly by asking each movie what types of movies they like. Then we can look at what movies they have rated and reverse engineer the features out of those movies.\n",
    "\n",
    "In mathematical terms, what feature vector should x(1) be so that theta(1)' * x(1) = 5 and theta(2)' * x1 = 5 and theta(3)' * x1 = 0 and theta(4)' * x1 = 0 ?\n",
    "\n",
    "- Optimization algorithm:\n",
    "Given theta(1), ... theta(n), to learn x(i):\n",
    "\n",
    "min(x(i)) /2 * SUM(j:r(i,j)=1) ((thetaj)' * x(i) - y(i,j)^2) + (lambda / 2) * SUM(k=1:n) (x(i)^2)\n",
    "\n",
    "- Using just ratings by users, you can randomly guess the inital thetas, and then learn the features.\n",
    "- - Wait, really? How.\n",
    "- Also, this only works well if users are very active and the matrix is fairly robust.\n",
    "\n",
    "### Collaborative Filtering Algorithm\n",
    "- An algorithm that can solve for theta and x simultaneously.\n",
    "- Dear god, that's a lot of summation.\n",
    "\n",
    "## Low Rank Matrix Factorization\n",
    "### Vectorization: Low Rank Matrix Factorization\n",
    "X = a matrix where every row is a movie, and every column is a feature of a movie.\n",
    "Theta = a matrix where every row is the parameter vector for a person, and every column is a parameter across all people.\n",
    "\n",
    "Y = X * Theta'\n",
    "\n",
    "- It's easy to measure how similar two movies are. We can just take the distance between the feature vectors for two movies.\n",
    "\n",
    "### Implementation Detail: Mean Normalization\n",
    "- What if there is a user who hasn't rated any movies?\n",
    "- We will only look at the regularization term, and so we will predict that this user will not like any movie.\n",
    "- - This is not necessarily helpful. What we can do instead is find the average rating of a movie across all users, and then shift Y so that each element is centered around the mean for that movie (subtract the mean from the value at Y).\n",
    "- - - This means that each movie in Y has mean 0.\n",
    "\n",
    "## Quiz\n",
    "Wow this quiz was hard... First try though.\n",
    "5/5 (100%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
