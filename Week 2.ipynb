{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "## Linear Regression with Multiple Variables\n",
    "### Multiple Features\n",
    "n = number of features\n",
    "*x^(i)* = input (features) of *ith* training example\n",
    "*Xj^(i)* = value of feature *j* in *ith* training example\n",
    "\n",
    "For example, *x(2)* = Vector '[1416 \\ 3 \\ 2 \\ 40]'\n",
    "*x(2, 3)* = 2\n",
    "\n",
    "### Gradient Descent for Multiple Variables\n",
    "Gradient descent equation is generally the same, but for n features instead of 1.\n",
    "Cost function:\n",
    "*J(w0, w1, ..., wn) = J(theta) = 1/2m SUM(i->m) ((h(x^(i)) - y^(i))^2*\n",
    "\n",
    "We will have to update all wi at the same time.\n",
    "\n",
    "### Gradient Descent in Practice 1 - Feature Scaling\n",
    " * Make sure features are on a similar scale.\n",
    "Example: x1 is size in feet (between 0 and 2000 feet^2)\n",
    "         x2 is number of bedrooms (1 - 5)\n",
    "         When mapping x1 and x2 on the same plane, contours of cost function will be very tall and skinny. This can slow the gradient descent.\n",
    " \n",
    "If we divide x1 by 2000 and x2 by 5, then both scales will be between 0 and 1.\n",
    "  * Gradient descent will converge much faster.\n",
    "\n",
    "Mean normalization:\n",
    "  * Subtract the mean of every feature from the instance of that feature, divide that by either the (max - min) or the standard deviation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
