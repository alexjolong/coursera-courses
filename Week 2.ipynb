{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "## Linear Regression with Multiple Variables\n",
    "### Multiple Features\n",
    "n = number of features\n",
    "*x^(i)* = input (features) of *ith* training example\n",
    "*Xj^(i)* = value of feature *j* in *ith* training example\n",
    "\n",
    "For example, *x(2)* = Vector '[1416 \\ 3 \\ 2 \\ 40]'\n",
    "*x(2, 3)* = 2\n",
    "\n",
    "### Gradient Descent for Multiple Variables\n",
    "Gradient descent equation is generally the same, but for n features instead of 1.\n",
    "Cost function:\n",
    "*J(w0, w1, ..., wn) = J(theta) = 1/2m SUM(i->m) ((h(x^(i)) - y^(i))^2*\n",
    "\n",
    "We will have to update all wi at the same time.\n",
    "\n",
    "### Gradient Descent in Practice 1 - Feature Scaling\n",
    " * Make sure features are on a similar scale.\n",
    "Example: x1 is size in feet (between 0 and 2000 feet^2)\n",
    "         x2 is number of bedrooms (1 - 5)\n",
    "         When mapping x1 and x2 on the same plane, contours of cost function will be very tall and skinny. This can slow the gradient descent.\n",
    " \n",
    "If we divide x1 by 2000 and x2 by 5, then both scales will be between 0 and 1.\n",
    "  * Gradient descent will converge much faster.\n",
    "\n",
    "Mean normalization:\n",
    "  * Subtract the mean of every feature from the instance of that feature, divide that by either the (max - min) or the standard deviation.\n",
    "  \n",
    "### Gradient Descent in Practice 2 - Learning Rate\n",
    "As gradient descent runs across many iterations, J(w)'s change will decellerate.\n",
    " * Declare convergence if J(w) decreases by less than 10^-3\n",
    "\n",
    "If J(w) is not decreasing, use a smaller value of alpha (a smaller learning rate).\n",
    " * It is mathematically proven that J(w) should decrease on every iteration for a sufficiently small alpha.\n",
    " * But a tiny alpha will be slow to converge\n",
    " \n",
    "Easy to choose a good alpha, just choose from .001, .003, .01, .03, .1, .3, 1, ... and plot J(w) to see which is best.\n",
    "\n",
    "### Features and Polynomial Regression\n",
    "Look at data points and decide whether changing the curve/behavior of the hypothesis function might better fit your data. Just add new features based on x to build quadratic, cubic, etc. functions.\n",
    " * Insight into the 'shape' of the data can lead to better models.\n",
    " * Important to use feature scaling for this, since your features might vary in range.\n",
    "\n",
    "In later sections, we will learn how to choose algorithms that will help you decide what features to use. I.e. whether you want to fit a quadratic function, cubic functions, etc.\n",
    "\n",
    "## Computing Parameters Analytically\n",
    "### Normal Equation\n",
    "Will give a much better value to solve for optimal value of theta for **some** linear regression problems.\n",
    "\n",
    "You can take the partial derivative of J(w) for every w, and set the result equal to 0. Then you can solve for all w0...n.  \n",
    "\n",
    "Design Matrix: Build an mX(n+1) matrix with all m instances of x, with all n features in x (and a column of 1s at the beginning).\n",
    "\n",
    "w = (X^T X)^-1 X^T y    with the above Design matrix X, and solution matrix y.\n",
    "\n",
    "No proof.. but it works.\n",
    "Gradient Descent\n",
    " * Need to choose alpha\n",
    " * Needs many iterations\n",
    " * Works well even when n is large\n",
    " \n",
    "Normal Equation\n",
    " * No need to choose alpha\n",
    " * Don't need to iterate\n",
    " * Need to compute (X^T X)^-1, which is a nXn matrix (n features, m training examples). Computing its inverse is O(n^3). So if n is very large, this will be slower than gradient descent on the same features.\n",
    " * There is no strict cutoff, but usually usually around n = 1000.\n",
    "\n",
    "Normal equation method does not work for classification algorithms like logistic regression. \n",
    "\n",
    "### Normal Equation Noninvertibility\n",
    "Only some matrices are invertible (i.e. singular / degenerate).\n",
    " * Could happen if there are redundant (linearly dependent) features in your matrix.\n",
    " * Might happen if there are too many features (if m <= n). This can be solved by using regularization, or deleting some features.\n",
    "\n",
    "Octave will compute the pseudo-inverse, not the inverse, if the matrix is singular. This will give you a result which works.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
