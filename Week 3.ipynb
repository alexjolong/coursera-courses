{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "## Classification and Representation\n",
    "### Classification\n",
    "Variable y is discrete-valued.\n",
    "For example:\n",
    " * Is email spam or not? (binary)\n",
    " * Is an online transaction fraudulent or not? (binary)\n",
    " * Is a handwritten character an A, B, C, ...? (multiclass)\n",
    "\n",
    "Assign y to one of two values:\n",
    "y -> {0,1}\n",
    "\n",
    "Can try to fit a straight line to data and then \"threshold\". If y is above j, the y is 1, otherwise it is 0.\n",
    " * However, the data may not fit linearly.\n",
    "\n",
    "Logistic regression is an algorithm which always predicts a value h(x) which is always within the bounds of the actual values of y.\n",
    "\n",
    "### Hypothesis Representation\n",
    "What is the function used to represent the hypothesis h(x)?\n",
    "h(x) = g(w^T x) (g of theta transpose x)\n",
    "\n",
    "Where g(z) = 1 / (1 + e^-z)\n",
    "^ This is also known as the sigmoid function or Logistic function.\n",
    " * The sigmoid function asymptotes at 0, and never produces a value < 0 or > 1.\n",
    "\n",
    "So,\n",
    "h(x) = 1 / (1 + e^-(theta^T * x))\n",
    "\n",
    " * h(x) is the estimated probability that y = 1 with input x.\n",
    " * For a regression model trained on tumor size where y is {benign, malignant}, if h(x) is 0.7, then the tumor given by x has a 70% chance of being malignant.\n",
    "\n",
    "hw(x) = P( y=1 | x; w)\n",
    "\n",
    "### Decision Boundary\n",
    "Set 0.5 as your boundary >= means y=1, < means y=0.\n",
    "When is h(x) = 0.5? \n",
    "g(z) = 0.5 when z = 0\n",
    "h(x) = g(w^T x) = 0.5 when w^T x = 0\n",
    "\n",
    "Example:\n",
    "hw(x) = g(w0 + w1x1 + w2x2)\n",
    "If w = \\[-3; 1; 1\\]\n",
    "Then y = 1 if -3 + x1 + x2 >= 0\n",
    "x1 + x2 >= 3\n",
    "\n",
    " * graph of x1 + x2 >= 3 is the Decision Boundary\n",
    " * decision boundary is a property of hypothesis, not the data\n",
    " * But training data affects the weights.\n",
    " \n",
    "5 - x1 >= 0.5 when x1 <= 4.5\n",
    "\n",
    "**Non-linear decision boundaries**\n",
    "hw(x) = g(w0 + (w1 * x1) + (w2 * x2) + (w3 * x1^2) + (w4 * x2^2))\n",
    "lets say w = \\[-1; 0; 0; 1; 1\\]\n",
    "So y = 1 if (-1 + x1^2 + x2^2) >= 0\n",
    "* x1^2 + x2^2 = 1 is a circle centered on the origin\n",
    "\n",
    "So decision boundary is a circle where everything inside will produce y = 0, everything outside will produce y = 1.\n",
    "\n",
    "**Higher Order polynomials**\n",
    " * Decision boundary might be an ellipse or more complex shapes.\n",
    "\n",
    "\n",
    "## Logistic Regression Model\n",
    "### Cost Function\n",
    "How to fit parameters w?\n",
    "We could use the same cost function as linear regression, taking the average of the squared errors from h(x) and y.\n",
    " * However, this produces a non-convex function due to the nonlinearity of the sigmoid function inside. Many local optima, and gradient descent doesn't work very well.\n",
    " \n",
    "Logistic Regression Cost Function:\n",
    "Cost(hw(x),y) = \n",
    "  -log(hw(x)) if y=1\n",
    "  -log(1 - hw(x)) if y=0\n",
    "\n",
    "IA negative logarithm approaches a minimum and levels out. So this works well as a cost function.\n",
    "As hw(x) -> 0, Cost -> inf\n",
    " * This penalizes false negatives very heavily. If we predict a very small probability, and we are wrong, that will cost us a lot.\n",
    "\n",
    "This was a helpful video.\n",
    "\n",
    "### Simplified Cost Function and Gradient Descent\n",
    "We can compress the two lines from the two cases above into one equation.\n",
    "Cost(hw(x),y) = -y * log(hw(x))  -  (1-y) log(1 - hw(x))\n",
    "\n",
    "Basically the (1-y) in the second term becomes 0 if y = 1, and we only use the first term. The \"-y\" in the first term is 0 if y = 0, and we use the second term.\n",
    "\n",
    "There are other cost functions we can use. This one is derived from \"Maximum likelihood estimation\" principle.\n",
    "\n",
    "We want to mimimize J(w). Update rule from Linear regression can be used.\n",
    "\n",
    "\n",
    "### Advanced Optimization\n",
    "We can scale much better to large datasets.\n",
    "Optimization algorithms:\n",
    " * Gradient descent\n",
    " * Conjugate gradient\n",
    " * BFGS\n",
    " * L-BFGS\n",
    "\n",
    "Advantages to above algorithms:\n",
    " * Clever inner-loop (line search algo) means you don't need to manually pick the learning rate.\n",
    " * Often converge faster than gradient descent (due to the above)\n",
    "Disadvantages:\n",
    " * More complex\n",
    "\n",
    "\n",
    "## Multiclass Classification\n",
    "### Multiclass Classification: One-vs-all\n",
    "May have n possible discrete values of y.\n",
    "\n",
    "One-vs-All / One-vs-rest Classification:\n",
    "Turn training set into n seperate binary classification problems. For the ith class, we find a decision boundary where we find whether or not x is in that class. We should (hopefully) find just one class that fits. We may have multiple, but we just choose the one with the highest probability.\n",
    "\n",
    "\n",
    "## Quiz \n",
    "5/5 (100%)\n",
    "\n",
    "*I'm honestly impressed with myself, didn't feel very confident in the material at all*\n",
    "\n",
    "\n",
    "## Solving the Problem of Overfitting (Friday)\n",
    "### The problem of overfitting\n",
    "For linear regression, fitting a straight line to data which might better match a quadratic function is \"underfitting\". Also, it will have a \"high bias\". \n",
    "\n",
    "Overfitting is the opposite, with \"high variance\". If you fit a high-order polynomial to data that would be suited to a quadratic function, you are introducing unnecessary variability.\n",
    "\n",
    "Too many features can fit the training set very well, but may not generalize to new data.\n",
    "\n",
    "How to address overfitting:\n",
    " * Two options:\n",
    " ** reduce the number of features. Possibly by manually selecting which features to keep. later in course - model selection algorithm\n",
    " ** Regularization - keep all features but reduce the magnitude of parameters w. This works well when we have a lot of features, each of which contributes a bit to predicting y.\n",
    "\n",
    "\n",
    "### Cost Function\n",
    "Regularizing cost function:\n",
    " * We want smaller values for parameters w0, w1, w1 ... wn\n",
    " * This will give us a \"simpler\" hypothesis - the function will be smoother.\n",
    " * So cost will be less prone to overfitting.\n",
    "\n",
    "Example:\n",
    " * training set with 100 features, 100 parameters. Hard to pick in advance which features will be relevent or not.\n",
    " * to your cost function, add the sum of all w^2 weighted heavily with the regularization parameter lambda.\n",
    " ** lambda captures the tradeoff between two goals: goal of the first term is to fit the training set well, second goal is to smooth the function.\n",
    " ** adding this sum will drive up the average cost and thus dampen the values of w except for those which contribute heavily to fitting the model.\n",
    " ** if lambda is too large, we will underfit (penalize too heavily).\n",
    "\n",
    "\n",
    "### Regularized Linear Regression\n",
    "For linear regression without regularization, we used gradient descent and updated w all at once.\n",
    "\n",
    "We can add the regularization for just one feature w by adding (lambda/m) * wj\n",
    "\n",
    "Because of math, we can just shrink wj a little (multiply by ~0.99) and then do our normal gradient descent.\n",
    "\n",
    "\n",
    "### Regularized Logistic Regression\n",
    "Add to logistic regression equation lamda/2m * SUM from 1-n of wj^2\n",
    "\n",
    "Cost function also includes the above term.\n",
    "\n",
    "Derivative with respect to theta0 does not change, but derivative with respect to other thetas does change, add *y/m * thetaj*\n",
    "\n",
    "Much more powerful nonlinear classifiers than polynomial regression. Learning these next!\n",
    "\n",
    "\n",
    "## Quiz\n",
    "5/5 (100%)\n",
    "\n",
    "## Programming Assignment (Saturday)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
