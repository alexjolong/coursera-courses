{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "## Classification and Representation\n",
    "### Classification\n",
    "Variable y is discrete-valued.\n",
    "For example:\n",
    " * Is email spam or not? (binary)\n",
    " * Is an online transaction fraudulent or not? (binary)\n",
    " * Is a handwritten character an A, B, C, ...? (multiclass)\n",
    "\n",
    "Assign y to one of two values:\n",
    "y -> {0,1}\n",
    "\n",
    "Can try to fit a straight line to data and then \"threshold\". If y is above j, the y is 1, otherwise it is 0.\n",
    " * However, the data may not fit linearly.\n",
    "\n",
    "Logistic regression is an algorithm which always predicts a value h(x) which is always within the bounds of the actual values of y.\n",
    "\n",
    "### Hypothesis Representation\n",
    "What is the function used to represent the hypothesis h(x)?\n",
    "h(x) = g(w^T x) (g of theta transpose x)\n",
    "\n",
    "Where g(z) = 1 / (1 + e^-z)\n",
    "^ This is also known as the sigmoid function or Logistic function.\n",
    " * The sigmoid function asymptotes at 0, and never produces a value < 0 or > 1.\n",
    "\n",
    "So,\n",
    "h(x) = 1 / (1 + e^-(theta^T * x))\n",
    "\n",
    " * h(x) is the estimated probability that y = 1 with input x.\n",
    " * For a regression model trained on tumor size where y is {benign, malignant}, if h(x) is 0.7, then the tumor given by x has a 70% chance of being malignant.\n",
    "\n",
    "hw(x) = P( y=1 | x; w)\n",
    "\n",
    "### Decision Boundary\n",
    "Set 0.5 as your boundary >= means y=1, < means y=0.\n",
    "When is h(x) = 0.5? \n",
    "g(z) = 0.5 when z = 0\n",
    "h(x) = g(w^T x) = 0.5 when w^T x = 0\n",
    "\n",
    "Example:\n",
    "hw(x) = g(w0 + w1x1 + w2x2)\n",
    "If w = \\[-3; 1; 1\\]\n",
    "Then y = 1 if -3 + x1 + x2 >= 0\n",
    "x1 + x2 >= 3\n",
    "\n",
    " * graph of x1 + x2 >= 3 is the Decision Boundary\n",
    " * decision boundary is a property of hypothesis, not the data\n",
    " * But training data affects the weights.\n",
    " \n",
    "5 - x1 >= 0.5 when x1 <= 4.5\n",
    "\n",
    "**Non-linear decision boundaries**\n",
    "hw(x) = g(w0 + (w1 * x1) + (w2 * x2) + (w3 * x1^2) + (w4 * x2^2))\n",
    "lets say w = \\[-1; 0; 0; 1; 1\\]\n",
    "So y = 1 if (-1 + x1^2 + x2^2) >= 0\n",
    "* x1^2 + x2^2 = 1 is a circle centered on the origin\n",
    "\n",
    "So decision boundary is a circle where everything inside will produce y = 0, everything outside will produce y = 1.\n",
    "\n",
    "**Higher Order polynomials**\n",
    " * Decision boundary might be an ellipse or more complex shapes.\n",
    "\n",
    "\n",
    "## Logistic Regression Model\n",
    "### Cost Function\n",
    "\n",
    "\n",
    "### Simplified Cost Function and Gradient Descent\n",
    "\n",
    "\n",
    "### Advanced Optimization\n",
    "\n",
    "\n",
    "\n",
    "## Multiclass Classification\n",
    "### Multiclass Classification: One-vs-all\n",
    "\n",
    "\n",
    "## Quiz \n",
    "\n",
    "## Solving the Problem of Overfitting (Saturday)\n",
    "### The problem of overfitting\n",
    "\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "\n",
    "### Regularized Linear Regression\n",
    "\n",
    "\n",
    "### Regularized Logistic Regression\n",
    "\n",
    "\n",
    "## Quiz\n",
    "\n",
    "\n",
    "## Programming Assignment (Sunday)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
