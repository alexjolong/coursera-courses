{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8\n",
    "## Clustering\n",
    "### Unsupervised Learning: Introduction\n",
    " - Unsupervised learning - finding structure in unlabeled data\n",
    "\n",
    "### K-Means Algorithm\n",
    " - First select \"cluster centroids\" randomly from the input data.\n",
    " - Assign all other data to a cluster based on how far they are from the center.\n",
    " - Compute the average of each cluster, this is your new centroid.\n",
    " - Repeat until cluster centroids to not change when mean is computed.\n",
    " \n",
    " - c(i) = index of cluster (1, 2, ..., K) to which example x(i) is currently assigned\n",
    " - mu(k) = cluster centroid k\n",
    "\n",
    "### Optimization Objective\n",
    " - For k-means, what is the cost function to optimize?\n",
    " - mu(c(i)) = cluster centroid of cluster to which x(i) have been assigned\n",
    "\n",
    "J(c(1), ..., c(m), mu(1), ..., mu(K)) = 1/m SUM(i=1:m)( ||x(i) - mu(c(i))||^2 )\n",
    "\n",
    "In words, we minimize the sum across all examples of the square distance between each point in a cluster and the cluster centroid.\n",
    "\n",
    "### Random Initialization\n",
    " - the use of random initialization means that k-means is susceptible to local optima.\n",
    "\n",
    "### Choosing the Number of Clusters\n",
    " - Most common way to choose the number of clusters is looking at visualization/otherwise doing it by hand.\n",
    " - Difficult because it is generally ambiguous.\n",
    "\n",
    "\"Elbow method\" of choosing the value of K\n",
    " - Plot the cost function J of K means with an increasing number of clusters.\n",
    " - There is sometimes a clear \"elbow\" in the curve - an inflection point where the cost funtion decreases more slowly.\n",
    " - There isn't always an elbow, so this method isn't used all the time.\n",
    " \n",
    "You can use a metric specific to your data by which you can choose the best value of K. For example, if you are trying to decide on t-shirt sizing, you might want K to be between 3 and 5.\n",
    "\n",
    "## Quiz\n",
    "4/5 (80%)\n",
    "\n",
    "\n",
    "## Motivation to Dimensionality Reduction\n",
    "### Motivation I: Data Compression\n",
    "Two features might mean basically the same thing (i.e. length in centimeters and length in inches). We can reduce this down from 2D to 1D, and reduce the redundancy. I.e. we don't want features to be dependant on each other.\n",
    "\n",
    "Another example: you have two different features, one measuring pilot skill and another measuring pilot attention. You might combine these into one feature which measures pilot \"aptitude\".\n",
    "\n",
    "You can do dimensionality reduction by projecting the two features onto a line which fits the data.\n",
    " - What happens if the features are not linearly related? I.e. plotting them together produces a exponential function.\n",
    "\n",
    "### Motivation II: Visualization\n",
    "Plotting data in R3 or higher can get confusing. Best to plot in R2, or R1.\n",
    "\n",
    "## Principal Component Analysis\n",
    "### Principal Component Analysis Problem Formulation\n",
    "PCA is the most common algorithm for dimensionality reduction.\n",
    " - First, apply mean normalization and feature scaling to the data.\n",
    " - Second, find a lower-dimensional surface onto which to project our data.\n",
    " -- In essence, find the k vectors which define a surface in Rk which most closely fits the data.\n",
    "\n",
    "PCA is NOT linear regression.\n",
    " - linear regression attempts to find a surface which is closest to y(i) for any input x(i)\n",
    " - PCA tries to find a surface which is the shortest distance from any input (x,y) (or however many dimesions).\n",
    "\n",
    "### Principal Component Analysis Algorithm\n",
    "Mean normalization:\n",
    "mu(j) = 1/m * SUM(i=1:m)(x(i,j))\n",
    "replace each x(i,j) with x(j) - mu(j)\n",
    "\n",
    "PCA algorithm:\n",
    "1. Compute covariance matrix\n",
    "Sigma = 1/m * SUM(i=1:n)(x(i)) * (x(i))'\n",
    "\n",
    "Compute eigenvectors of matrix Sigma (syntax \\[U,S,V\\] = svd(Sigma))\n",
    " - SVD stands for singular value decomposition\n",
    " - There is also eig(Sigma), but when applied to a covariance matrix they do the same thing.\n",
    "\n",
    "U matrix that results from svd will be nxn, and the columns will be the vectors that we want. In order to reduce the dimensions down to k, we just pick the first k columns of U.\n",
    "\n",
    " - Side note: I really need to brush up on my linear algebra. I don't remember what an eigenvector is, conceptually.\n",
    "\n",
    "Our first k vectors become a matrix called Ureduce. Set z = Ureduce' * x.\n",
    "z is kxn * nx1 = kx1\n",
    "\n",
    "### Reconstruction from Compressed Representation\n",
    "Xapprox = Ureduce * z\n",
    "Xapprox is nxk * k* 1 = nx1\n",
    "\n",
    "If the squared projection error is not too big, then Xapprox should be close to what x was before reduction. \n",
    "\n",
    "### Choosing the Number of Principal Component\n",
    "How should you choose K?\n",
    " - Choose the smallest value so that the ratio between the average squared projection error and the total variation in the data is less than 1%. This means that 99% of the variance is retained.\n",
    " - When we compute svd, we get matrix S. This is a square diagonal matrix with the diagonal elements are S11, S22, ... Snn.\n",
    " \n",
    "The variance can be computed using Sii using 1 - (SUM(i=1:k)Sii / SUM(i=1:n)Sii). Flip 1 over and get your fraction >= 0.99 for some value of k.\n",
    "\n",
    "### Advice for Applying PCA\n",
    " - Using PCA to speed up the running time of a learning algorithm\n",
    " - This is actually the most common use of PCA\n",
    " \n",
    "A learning algorithm can be very slow when you have very high dimensional feature vectors.\n",
    "\n",
    "1. Extract inuts from a labeled dataset and place them in a matrix.\n",
    "2. Apply PCA to reduce the dimensionality. This will give you a new matrix with the same number of examples but with fewer features.\n",
    "3. Build your new training set (z(1), y(1)), (z(2), y(2)), ..., (z(m), y(m))\n",
    "\n",
    "Applications of PCA:\n",
    " - Compression. Useful for reducing memory/disk needed, and for speeding up learning algorithms.\n",
    " - Visualization.\n",
    "\n",
    "Don't use PCA to prevent overfitting. It may work, since fewer features = less likely to overfit. However, regularization is a much better tool. \n",
    " - This is because regularization knows what the values y are, and is less likely to throw away valuable information.\n",
    "\n",
    "Don't use PCA when you don't have to! Consider implementing your model without PCA first, and them use PCA if it doesn't work at first.\n",
    " - Again, this prevents you from getting rid of meaningful data on accident.\n",
    "\n",
    "\n",
    "## Quiz\n",
    "4/5 (80%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
