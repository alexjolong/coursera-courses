{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7\n",
    "## Large Margin Classification\n",
    "### Optimization Objective (14)\n",
    "I'm actual excited to learn about support vector machines because I want to do work with generative neural networks.\n",
    "\n",
    "SVMs are sometimes cleaner and more powerful at learning complex nonlinear functions.\n",
    "\n",
    "Optimization objective:\n",
    "\n",
    "Take logistic regression, except replace the (-log(h(xi))) term with cost1(theta' xi) and cost0(theta' xi) accordingly.\n",
    "\n",
    "SVM doesn't output probability, it gives 1 if theta'x >= 0 or 0 otherwise.\n",
    "\n",
    "### Large Margin Intuition (10)\n",
    "An SVM is aka a \"Large Margin Hypothesis\"\n",
    "\n",
    "- SVM tries to seperate data from decision boundary with the largest margin possible.\n",
    "\n",
    "Large margin classifier learning outcomes can be highly sensitive to outliers.\n",
    "\n",
    "### Mathematics Behind Large Margin Classification (19)\n",
    "The optimization function is minimizng the squared norm of the squared length of the vector theta.\n",
    "\n",
    "\n",
    "## Kernels\n",
    "### Kernels I (15)\n",
    "Feature \"landmarks\" l1, l2, l3, ...\n",
    "f1 = similarity(x,l1)\n",
    "- f1 is the similarity between x and landmark1, where the function is...\n",
    "epx(- ||x -l1||^2 / 2sigma^2)\n",
    "- This is the euclidean distance between x and l1, divided by two * sigma^2\n",
    "\n",
    "This particular similarity function makes f1 a Gaussian kernel.\n",
    "- If x is very close to l1, then f1 will be close to 1\n",
    "- If x is very far from l1, then f1 will be close to 0\n",
    "- The surface of f1 for all values x is exponential, depending on the value of sigma. If sigma is larger, the slope of the curve will be more gradual.\n",
    "- Just like a large (1/lambda) value equates to high variance and low bias, and a small (1/lambda) value equates to high bias and low variance, \n",
    "- A large sigma will lead to higher bias and lower variance, and a small sigma will lead to lower bias and higher variance.\n",
    "\n",
    "### Kernels II (15)\n",
    "How do we define the landmarks?\n",
    " - Put landmarks at the same locations as some training examples.\n",
    " - Compute the similarity of every location and the landmark (where sim(xi,xi) is 1).\n",
    " - Put the results in a new feature vector fi\n",
    "\n",
    "\n",
    "## SVMs in Practice\n",
    "### Using an SVM (21)\n",
    "- There are many software packages which you can use (liblinear, libsvm, etc.)\n",
    "- You need to choose/specify: Choice of parameter C, choice of kernel (similarity function)\n",
    "- \"No Kernel\" means \"linear kernel\" - i.e. (theta0 + theta1\\*x1 + ... + thetan\\*xn) >= 0\n",
    "- If you use a Gaussian kernel, you should apply feature scaling to the inputs\n",
    "- Kernels need to satisfy \"Mercer's Theorem\" to ensure that optimization converges. \n",
    "-- Polynomial Kernel\n",
    "-- String Kernel\n",
    "-- Chi-square kernel\n",
    "-- Histogram intersection kernel\n",
    "\n",
    "- Many SVM pacakges support multi-class classification\n",
    "- You can always use ones-vs-all method (Train K SVMs for k = # classes)\n",
    "\n",
    "- In general, use logistic regression or linear kernel SVM when the number of features is very large relative to the number of training examples (i.e. spam emails with a lot of features (words) but few examples).\n",
    "- If the number of features is small and the number of training examples is intermediate, use SVM with Gaussian Kernel\n",
    "- If the number of features is small and the number of training examples is large, use logistic regression or SVM without a kernel. This is because Gaussian SVM will be slow to train. If you can, add more features to prevent high bias / underfitting.\n",
    "\n",
    "\n",
    "## Quiz\n",
    "5/5 (100%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
