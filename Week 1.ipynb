{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "## Model and Cost Function\n",
    "### Model Representation\n",
    "\n",
    "Using *(x(i), y(i))* to denote *ith* item of a set, where *x(i)* is the *ith* input variable and *y(i)* is the *ith* output variable.\n",
    "\n",
    "Also, *i* in this context counts from 1\n",
    "\n",
    "y = h(x)\n",
    " * h originally stood for \"hypothesis\"\n",
    "\n",
    "Univariate linear regression = just one variable.\n",
    "\n",
    "X and Y denote the space of input and output values, respectively.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "*(hw)(x) = (w0) + (w1)x*\n",
    " * w's are used here in place of 'theta'. *hw* is the hypothesis at the value set theta.\n",
    " \n",
    "Minimization problem:\n",
    "Minimize w0 w1. \n",
    "\n",
    "Solution:\n",
    "choose weights such that the hypothesized value of *hw(x)* is close to y for the training exmaples *(x,y)*. I.e. pick weights that give you a similar output as *hw(x)*\n",
    "sum from 1 to m of *(hw(x(i)) - y(i))^2*\n",
    " * When m is the number of your training examples, this will give you the square of the total difference between the trained solution's output and hypothetical solution's output for all training inputs. You want to minimize this.\n",
    " * However, to make things easier, divide it by 2m. This will give you a smaller number but minimizing it still works as a heuristic.\n",
    " * In other words, \"find me the weights which cause the differences between their formula output and the real example's output is minimized\"\n",
    " \n",
    "This is also known as the Squared Error Function.\n",
    " * Why do we square? My intuition is to minimize smaller errors and maximize larger ones. Basis: wider margins of error are proportionally worse to medium margins than medium are to small.\n",
    " \n",
    "### Cost Function Intuition 1\n",
    "Ahh, we also square because we are minimizing the magnitude of the error, not the error itself (or we would make our weights negative for positive inputs). Why not just take the max value? Perhaps this is less computationally expensive.\n",
    "\n",
    "J(0) = 1/6 ((0 - 1)^2 + (0 - 2)^2 + (0 - 3)^2)\n",
    "J(0) = 1/6 (1 + 4 + 9)\n",
    "J(0) = 14/6\n",
    "\n",
    "### Cost Function Intuition 2\n",
    "I understand everything now.\n",
    "\n",
    "The 3D cost function for 2 weights is super dope. I wish I had these types of visuals when I was learning calculus.\n",
    "\n",
    "## Parameter Learning\n",
    "### Gradient Descent\n",
    "A general algorithm, good at minimizing a function *J(w0, w1)*. Works for greater dimensional equations as well.\n",
    "\n",
    "*min J(w1, ..., wn)*\n",
    "\n",
    "Steps:\n",
    "1. Start with some w0, w1\n",
    "2. Keep changing w0, w1 to reduce *J(w0, w1)* until we hopefully end up at a minimum (hopefully not a local one!)\n",
    "\n",
    "\"For the value of the coordinates I am at (values of w), and the values of w surrounding me, which is smallest?\"\n",
    "\n",
    "*wj := wj - a(d/d(wj) J(w0, w1))* (for j=0 and j=1)\n",
    " * a := b means assign the value of b to 1.\n",
    " * a = learning rate (the amount of step we take. Large alphas = large steps).\n",
    " * (d/d(wj)) is a derivative in terms of wj\n",
    " * Note that because we are taking the derivative of both j=1 and j=0, we need to simultaneously update wj (both w0 and w1). This 'simultaneousness' means that we cannot update w0 before it is used in the equation to calculate w1. Otherwise the algorithm might behave strangely.\n",
    "\n",
    "temp0 = 1 + sqrt(2)\n",
    "temp1 = 2 + sqrt(2)\n",
    "w0 = temp0 = 1 + sqrt(2)\n",
    "w1 = temp1 = 2 + sqrt(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
