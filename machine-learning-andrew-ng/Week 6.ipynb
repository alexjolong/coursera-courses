{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "## Evaluating a Learning Algorithm\n",
    "### Deciding What to Try Next\n",
    "Usually a difference between someone who knows the math but doesn't really understand how to apply it, and those who deeply understand it and know how to apply it to real-world problems (that's me).\n",
    "\n",
    "We're gonna go through some examples on how to apply the tools we have learned.\n",
    "\n",
    "Debugging a learning algorithm\n",
    " - More training examples (not always as helpful as you might think)\n",
    " - Try smaller sets of features (to prevent overfitting)\n",
    " - Try gathering additional features (another type of gathering more data, may be a large project and you can't be sure how helpful it will be beforehand).\n",
    " - Add polynomial features (x^2, x1x2, etc.)\n",
    " - Decrease / increase regularization lambda\n",
    "\n",
    "A lot of times, people will go by gut feeling when deciding how to debug. This wastes time and money.\n",
    "\n",
    "Machine learning diagnostic:\n",
    " - a test to run to gain insight into what is/isn't working.\n",
    " - These can take time to implement, but will usually save more time by preventing you from going down the wrong path.\n",
    "\n",
    "### Evaluating a Hypothesis\n",
    "Overfitting - just because a hypothesis works well to predict training examples, doesn't mean it will generalize.\n",
    "\n",
    "Split training data into training and test sets. (usually something like a 70% / 30% split). Best to randomize which examples are in each set. After a number of training iterations, figure out the error on the test sets and make sure it is still nondecreasing.\n",
    "\n",
    "Misclassification error: err(hOfX, y) = \n",
    "{1 if hOfX >= 0.5, y=0\n",
    "{1 if hOfX < 0.5, y=1\n",
    "{0 otherwise\n",
    "\n",
    "Test error = 1/m SUM(err(hOfXi, yi)) for all i\n",
    "\n",
    " - Presumably, you can also adapt misclassification errors to n output classes.\n",
    "\n",
    "### Model Selection and Train/Validation/Test Sets\n",
    "How to decide what degree of polynomial to fit to a dataset? What features should you include? How do you choose the right lambda.\n",
    "\n",
    "- These are called model selection problems\n",
    "\n",
    "To decide on a polynomial degree for your model, you could do it the naive way and train on each different model and plot the test set error of each, picking the set which minimizes this value.\n",
    " - How do you know that another model wouldn't do better with more training iterations, or perform better on a general data set?\n",
    " - We actual chose the value d (polynomial degree) which fits best to the test set. We may have overfit!!\n",
    " \n",
    "Instead, let's split the original training set into three sets - training set, cross validation set, and test set. Usually pick a ratio like 60%/20%/20%\n",
    " - Now we can do the above steps, except use the cross validation set to test the performance of each different degree model. Pick the set with the lowest cross validation error, and then continue with your training using the test set to measure the generalization error of this model.\n",
    " - We will expect the performance of the model on the cross validation set to perform better on the training dataset, but we will get a more accurate idea of our performance.\n",
    "\n",
    "## Bias vs Variance\n",
    "### Diagnosing Bias vs Variance\n",
    "Training error tends to decrease as polynomial degree of the model decreases (as we start to possibly overfit).\n",
    "\n",
    "Cross validation error is parabolic - it decreases as polynomial degree increases, but increases once polynomial degree starts to overfit to the training set and not the cross validation set.\n",
    "\n",
    "If training error and cross validation error are high, you have a \"high bias\" problem. If cross validation is high and training error is low, you have a \"high variance\" problem.\n",
    "\n",
    "### Regularization and Bias/Variance\n",
    "Similar to bias/variance with polynomial degrees model section, but this time for choice of lambda.\n",
    "\n",
    "Large lambda can result in high bias (underfit). Each of the thetas will approach 0 as lambda increases, and so our cost function will be flat.\n",
    "\n",
    "Small lambda can result in high varias (overfit). This is basically no regularization, so performs just like a high-order polynomial fit to the data.\n",
    "\n",
    "Intermediate lambda gives us a reasonable fit to the data.\n",
    "\n",
    "How to choose lambda:\n",
    " - Don't use regularization when calculating training, validation, and test set errors.\n",
    " - Try training models with lambdas in the range \\[0, 10\\]. Your lambdas should increase exponentially - i.e. 0, 0.1, 0.2, 0.4, 0.8, ... 10.\n",
    " - Use cross validation set to validate the error for each model, and pick the model that minimizes the error.\n",
    " - How well does J(theta) for this model perform on the test set?\n",
    " - Can you do this at the same time as picking polynomial degree? Are their any dependencies?\n",
    "\n",
    "### Learning Curves\n",
    "Plot training set and cross validation set errors for linearly increasing training set sizes.\n",
    "\n",
    "Usually training error will increase logarithmically (initially fits very well, and will begin to level out over time).\n",
    "\n",
    "Usually cost function error will decrease exponentially (initial high error, decreases and levels out).\n",
    "\n",
    "When you have a high bias, your training and cross validation errors will approach each other and level off at a relatively high error value. Note that more training data will not help very much, since as more data is added you don't get very good returns on your error.\n",
    "\n",
    "When you have a high variance, there will be a gap between your training and cross validation errors. Your training set will be overfit, resulting in a low training error. Your cross validation error will be high, but might continue to decrease with a higher training set size. In this scenario, it may help to get more data to train your model on.\n",
    "\n",
    "### Deciding What to Do Next Revisited\n",
    "Debugging a learning algorithm:\n",
    " - More training examples (this will help if you have a high variance problem. i.e. if you are overfitting to the training set)\n",
    " - Smaller sets of features (this will also help high variance problems and will prevent overfitting)\n",
    " - Adding more (or higher order) features (this will help high bias problems. If your current hypothesis is too simple, it may help to add more higher-order features)\n",
    " - Decreasing lambda (this will fix high bias problems)\n",
    " - Increasing lambda (this will fix high variance problems)\n",
    "\n",
    "Neural network-specific issues:\n",
    " - Small networks have fewer parameters and so are more prone to underfitting. They are, however, computationally cheaper.\n",
    " - Large networks have more parameters and so are more prone to overfitting. They are more computationally expensive.\n",
    " - You can use the cross validation technique to select the number of hidden layers to use in a neural network.\n",
    "\n",
    "## Quiz\n",
    "5/5 (100%)\n",
    "\n",
    "## Building A Spam Classifier\n",
    "### Prioritizing what to work on\n",
    "How to build a spam classifier\n",
    "1. Supervised learning: x features of email, y=spam(1) or not(0)\n",
    "2. features x: Choose 100 words indicative of spam or not\n",
    "3. Feature vector would have dimension 100\n",
    "4. Usually you pick the most frequently occuring words for your features.\n",
    "\n",
    "How do you get a low error on a spam classifier?\n",
    "- Collect lots of data. This will help sometimes, but not all the time.\n",
    "- Develop sophisticated features based on email routing information\n",
    "- Developed more sophisticated features based on the email message.\n",
    "-- for example, can we combine \"discount\" and \"discounts\" into the same feature? Can we correct misspellings?\n",
    "\n",
    "Even people who have spent a lot of time working on the spam problem, don't know which option is best to explore. Far too many try to pick the solution rather than try to solve the problem in the most effective way.\n",
    "\n",
    "### Error Analysis\n",
    "Start with a simple algorithm that is simple to implement (i.e. < 1 day). Use it to get some errors on cross-validation data.\n",
    "\n",
    "Plot learning curves and use it to pick an approach: more data, more features, etc. See if you can spot trends in the examples that your algorithm failed on, and then address those shortcomings.\n",
    "\n",
    "Important to have a way to evaluate the performance of your algorithm. \n",
    "\n",
    "\n",
    "## Handling Skewed Data\n",
    "### Error Metrics for Skewed Classes\n",
    "What if one of the classes is very rare? We may want to minimize false positives (we predicte true but it was actually false) and false negatives (we predicted false but it was actually true).\n",
    "\n",
    "Precision = true positives / predicted positives = true positives / (true positives + false positives)\n",
    "\n",
    "Recall = true positives / actual positives = true positives / (true positives + false negatives)\n",
    "\n",
    "We can use precision and recall, rather than just a learning rate, to evaluate the performance of an algorithm with skewed classes.\n",
    "\n",
    "\n",
    "### Trading Off Precision and Recall\n",
    "Higher precision leads to lower recall (we will only delvier cancer diagnoses when we are very confident, but we may miss some people)\n",
    "\n",
    "Higher recall leads to lower precision.\n",
    "\n",
    "However, the relationship may not be linear. Lemme guess... you can plot these?\n",
    "\n",
    "- Taking the average of precision and recall is usually not very good.\n",
    "- F1 score (or just \"F Score\"): 2(PR/(P+R))\n",
    "- Since f1 takes the product, if either precision or recall are very low, you won't get a good score. Makes it good at striking a balance.\n",
    "- Also, a perfect F score is \"1\".\n",
    "\n",
    "\n",
    "## Using Large Data Sets\n",
    "How much data should you train on?\n",
    "\n",
    "- Some algorithms which don't perform well on small training set sizes perform great on larger training set sizes.\n",
    "- Can a human expert confidently predict y given x? If so, then a very large training set is likely to immprove the algorithm. This stems from the fact that if a human expert can predict y, then there are probably enough features in x such that we have low variance.\n",
    "\n",
    "I'm not sure if I follow that argument. A human expert may be able to sort through many features, but an algorithm might have high variance due to overfitting. Any way, I like the common sense of picking a complete set of features such that you yourself could solve the problem with the given info.\n",
    "\n",
    "## Quiz 2\n",
    "5/5 (100%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
