{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Scale Machine Learning\n",
    "## Gradient Descent with Large Datasets\n",
    "### Learning with Large Datasets\n",
    "* Best results usually come from low bias algorithms trained on a lot of data.\n",
    "\n",
    "What are some more efficient ways for computing gradient descent rather than iterating across all of the examples? Especially when our data set is very large (in the millions)\n",
    "* You can use smaller batches to see how it is performing along the way.\n",
    "* You can probably stop adding more data once Jcv and Jtrain start to converge on the same error value. If they don't you have high variance and will need to fix that.\n",
    "\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "* With a large dataset, it is computationally expensive to compute the cost function.\n",
    "* This is because we have to sum over all examples at every step of gradient descent.\n",
    "\n",
    "Stochastic gradient descent steps:\n",
    "1. Randomly shuffle dataset\n",
    "2. Repeat for all training examples:\n",
    "3. update theta(j) with the slope of the error at that example.\n",
    "\n",
    "Rather than finding a derivative for the cost of everything, it does it for every examples.\n",
    "* I'm guessing this makes it more wobbly, but since each step is smaller we probably get to the same place. \n",
    "* Randomly shuffling is really important.\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "* if batch gradient descent uses all m examples in each iteration and stochastic gradient descent uses 1 example in each iteration, then mini-batch gradient descent uses b examples in each iteration where 1<=b<=m\n",
    "\n",
    "### Stochastic Gradient Descent Convergence\n",
    "* Every thousand or so iterations, plot your performance (cost) to make sure it is decreasing.\n",
    "* If the line is too noisy, you can increase the number of examples you average over.\n",
    "\n",
    "## Advanced Topics\n",
    "### Online Learning\n",
    "Online learning algorithms are essentially stochastic, since you calculate for one user at a time.\n",
    "* Beneficial in some cases, because it can adapt to changing user preferences.\n",
    "\n",
    "\n",
    "### Map Reduce and Data Parallelism\n",
    "* Look up Jeff Dean - Map Reduce\n",
    "Multiple machines can divide up the work to compute one batch gradient and then combine them.\n",
    "\n",
    "\n",
    "## Quiz\n",
    "5/5 (100%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
